<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=OPeqXG-QxW3ZD8BtmPikfA');.lst-kix_fsnhrsdttzlh-3>li:before{content:"\0025cf  "}.lst-kix_fsnhrsdttzlh-4>li:before{content:"\0025cb  "}.lst-kix_fsnhrsdttzlh-2>li:before{content:"\0025a0  "}.lst-kix_fsnhrsdttzlh-6>li:before{content:"\0025cf  "}.lst-kix_fsnhrsdttzlh-0>li:before{content:"\0025cf  "}.lst-kix_fsnhrsdttzlh-7>li:before{content:"\0025cb  "}.lst-kix_fsnhrsdttzlh-8>li:before{content:"\0025a0  "}.lst-kix_fsnhrsdttzlh-1>li:before{content:"\0025cb  "}ul.lst-kix_fsnhrsdttzlh-7{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-8{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-5{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-6{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-3{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-4{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-1{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-2{list-style-type:none}ul.lst-kix_fsnhrsdttzlh-0{list-style-type:none}.lst-kix_fsnhrsdttzlh-5>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Roboto";font-style:normal}.c8{background-color:#ffffff;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none;font-size:9.5pt}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c30{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;text-align:left}.c6{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c20{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c23{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c28{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;text-align:left}.c15{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c16{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#333333;text-decoration:underline}.c12{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c22{-webkit-text-decoration-skip:none;color:#337ab7;text-decoration:underline;text-decoration-skip-ink:none}.c29{color:#666666;font-size:15pt;font-style:normal}.c4{font-size:10pt;font-family:"Roboto";font-weight:400}.c25{background-color:#ffffff;max-width:540pt;padding:36pt 36pt 36pt 36pt}.c27{text-decoration:none;vertical-align:baseline}.c0{color:inherit;text-decoration:inherit}.c21{width:33%;height:1px}.c5{font-weight:400;font-family:"Roboto"}.c11{color:#1f1f1f;font-weight:700}.c24{background-color:#ffffff;font-size:9.5pt}.c13{color:#1f1f1f}.c18{color:#333333}.c14{font-style:italic}.c17{font-size:26pt}.c19{font-size:16pt}.c9{font-size:10pt}.c26{font-size:20pt}.c3{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c25"><p class="c28 title" id="h.v6x1fhm1s4qm"><span class="c15 c5 c17">Machine Learning Engineer Nanodegree</span></p><p class="c30 subtitle" id="h.5d4qhprmbcdq"><span class="c27 c5 c29">Capstone Proposal</span></p><p class="c1"><span class="c2">Nathaniel Watkins</span></p><p class="c1"><span class="c2">July 12th, 2018</span></p><h1 class="c23" id="h.j63p9xoe1jwe"><span class="c15 c5 c26">Proposal: Kaggle Competition - Google AI Open Images - Object Detection Track</span></h1><p class="c1"><span class="c2">Competition:</span></p><p class="c1"><span class="c12 c5"><a class="c0" href="https://www.google.com/url?q=https://www.kaggle.com/c/google-ai-open-images-object-detection-track&amp;sa=D&amp;ust=1531459190681000">https://www.kaggle.com/c/google-ai-open-images-object-detection-track</a></span></p><p class="c1"><span class="c2">Dataset:</span></p><p class="c1"><span class="c12 c5"><a class="c0" href="https://www.google.com/url?q=https://storage.googleapis.com/openimages/web/index.html&amp;sa=D&amp;ust=1531459190681000">https://storage.googleapis.com/openimages/web/index.html</a></span></p><h2 class="c6" id="h.s3dzs44w865z"><span class="c15 c5 c19">Domain Background</span></h2><p class="c1"><span class="c2">Computer Vision (CV) is getting better and better every day, especially when focused on narrow tasks, such as detecting skin cancer; however, CV is still orders of magnitude weaker than humans when it comes to interpreting everything going on in a given picture.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 264.53px; height: 332.50px;"><img alt="" src="images/image1.png" style="width: 264.53px; height: 332.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c5">For example, while CV can reliably label the Mountain Goat in the picture on the left</span><sup class="c5"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c5">, I&rsquo;m sure most people would be able to identify several other things in it, such as a cloudless sky, boulders, bushes, and more. &nbsp;In order for Machine Learning to be able to benefit people on a whole new level, it&rsquo;ll need a much more comprehensive CV system that can extract almost all the important details out of a frame as seen in the picture below</span><sup class="c5"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c2">:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 347.50px; height: 347.50px;"><img alt="" src="images/image5.png" style="width: 347.50px; height: 347.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c2">I would like to play a part in driving the state-of-the-art in CV, thus enabling new, life-improving applications, some that we can&rsquo;t currently predict. &nbsp;Just like no one really predicted that combining GPS and a ubiquitous data connection in a device that people always had with them would lead to transportation revolutions like Uber and Lyft, I see this challenge like adding GPS to phones, and I want to help make it happen!</span></p><h2 class="c6" id="h.6a6m8txu8vur"><span class="c15 c5 c19">Problem Statement</span></h2><p class="c1"><span class="c5">In most CV applications, the model is trained to output only 1 result (or sometimes a confidence list), thus only being able to handle pictures with just 1 subject. &nbsp;But we all know that real world applications are messy and many pictures (or video feeds) will have multiple items that could be considered a subject, or worse yet the CV model might completely miss the point and focus on an irrelevant part of the picture. &nbsp;For example, see the picture to the right</span><sup class="c5"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c2">&nbsp;where a model trained to recognize dog breeds fixated on a spot of the floor thinking there was a human there, and completely ignored the cat that was the subject of the picture.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 367.72px; height: 314.50px;"><img alt="" src="images/image6.png" style="width: 367.72px; height: 314.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">We can try to solve this problem by training a CV model on a large dataset that has been marked up with bounding boxes for multiple classes by professional human annotators, and success will be measured by calculating the mean Average Precision of the model compared to the ground-truth annotations on a withheld test data set.</span></p><h2 class="c6" id="h.meyvk0y62anp"><span class="c15 c5 c19">Datasets and Inputs</span></h2><p class="c1"><span class="c5">The main dataset for this project is going to be the Open Images V4 dataset</span><sup class="c5"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c5">, provided by Google Inc. under the </span><span class="c12 c5"><a class="c0" href="https://www.google.com/url?q=https://creativecommons.org/licenses/by/4.0/&amp;sa=D&amp;ust=1531459190683000">CC by 4.0 License</a></span><span class="c5">. &nbsp;Plus, I plan on using a ConvNet that was pretrained on a dataset like ImageNet</span><sup class="c5"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup><span class="c5">&nbsp;or COCO</span><sup class="c5"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c5">, such as Inception-v3</span><sup class="c5"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c5">&nbsp;or YOLO-v3</span><sup class="c5"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span class="c2">, for transfer learning. &nbsp;All of these are publicly and freely available.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c5">This dataset is perfect for this task since it contains about 9 million images with a special subset of 1.74M images featuring 14.6M bounding boxes for 600 object classes. &nbsp;Not only is this the largest collection of images with object location annotations, but they were mostly drawn by professional annotators for the highest quality and the machine drawn boxes were all human verified. &nbsp;Here is a heirarchy diagram of all 600 boxable classes: </span><span class="c5 c12"><a class="c0" href="https://www.google.com/url?q=https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy_visualizer/circle.html&amp;sa=D&amp;ust=1531459190684000">https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy_visualizer/circle.html</a></span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">The Open Images V4 dataset will be obtained directly from Kaggle who is hosting the competition and used to train an ImageNet pre-trained ConvNet (obtained as a native Application from Keras). &nbsp;This competition was designed specifically to push the state-of-the-art in CV using this new dataset. &nbsp;I plan on using cloud computing to power through training on such a large dataset, either using Kaggle Kernels or in a Google Cloud TPU. &nbsp;So that the reviewer doesn&rsquo;t need to download 561GB of data, I&rsquo;ll start the notebook by exploring some samples and include a smaller subset of the dataset in my submission.</span></p><h2 class="c6" id="h.mitybg1xajal"><span class="c15 c5 c19">Solution Statement</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 361.50px; height: 272.69px;"><img alt="" src="images/image2.png" style="width: 361.50px; height: 272.69px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><p class="c1"><span class="c5">This problem can be solved by generating predictions and bounding boxes that closely match what the human annotators have created. &nbsp;The labels are organized in a semantic hierarchy, so the model will need to output not just the object classes (such as &lsquo;Mango&rsquo;), but also any relevant parent classes (&lsquo;Fruit&rsquo; and &lsquo;Food&rsquo;). &nbsp;Since the label data in this set is non-exhaustive, False Positives that do not coincide with any labeled classes for that image will be ignored, except in the case of a detection that lines up with a negative label on an image, such as an image with a cat that was labeled specifically as not containing a dog. (see example to the left</span><sup class="c5"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span class="c2">). &nbsp;Furthermore, many images contain groups of a class (5+ instances that are touching and occluding each other), so a True Positive can be counted if there is at least 1 correct detection of that class with a bounding box &nbsp;that is at least halfway inside the &lsquo;Group of&rsquo; box. &nbsp;I&rsquo;ll replicate this behavior with a separate validation dataset, and the competition will eventually test my model with a withheld test group.</span></p><h2 class="c6" id="h.flydj0f8ukrh"><span class="c15 c5 c19">Benchmark Model</span></h2><p class="c1"><span class="c2">To solve this problem, I plan on starting with a starter notebook and using it as the benchmark model, like the ResNet-50 model linked below, then building on it by tuning the parameters and model structure, before trying a more cutting edge algorithm to substantially improve on the mAP performance from that baseline model. &nbsp;This would provide a direct comparison to measure the amount of improvement that I was able to provide throughout the course of this project. &nbsp;The linked model is already able to achieve the solution, as laid out earlier, with a reasonable degree of accuracy, but seems to have a bug in outputting the coordinates to the Prediction String (of course I would ensure that the final benchmark model I settle on would be bug free). &nbsp;A random choice model would statistically result in a near 0 mAP score, (higher is better; see Evaluation Metrics below for more details). &nbsp;And since a 0 mAP would not be very helpful in determining relative progress, I&rsquo;ll compare my results to the the best results I can find when starting this journey.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c12 c5"><a class="c0" href="https://www.google.com/url?q=https://www.kaggle.com/shivamb/objects-bounding-boxes-using-resnet50-imageai&amp;sa=D&amp;ust=1531459190685000">https://www.kaggle.com/shivamb/objects-bounding-boxes-using-resnet50-imageai</a></span></p><h2 class="c6" id="h.bcq3nzgsn2l9"><span class="c15 c5 c19">Evaluation Metrics</span></h2><p class="c1"><span class="c5">The mean Average Precision (mAP) is simply the mean of the Average Precision (AP) for each of the 500 classes , which will be the final ranking metric for the competition and my means of quantifying progress. &nbsp;A simple AP is calculated as the current sum of every correct detection over the sum of all instances of that class encountered so far, but this competition is using the definition of AP set forward on page 10 of PASCAL VOC 2010</span><sup class="c5"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span class="c2">, which is basically the area under the Precision/Recall curve. &nbsp;These values are only calculated based on classes tagged in images (either positive or negative), so if a class is not present in an image, False Positives get ignored. &nbsp;This method of evaluation seems to be the most practical way to measure the relevance of the results condensed down into one number, and thus allowing a fair comparison of the relative performance between models.</span></p><h2 class="c6" id="h.89412zmd1r7q"><span class="c15 c5 c19">Project Design</span></h2><p class="c1"><span class="c5">My primary goal for this project will be to figure out how to implement at least one of the top algorithms representing the state-of-the-art in CV and Object Detection today, such as YOLO, SSD</span><sup class="c5"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span class="c5">&nbsp;or Faster R-CNN</span><sup class="c5"><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span class="c2">&nbsp;and thus learn more about what makes it tick. &nbsp;Furthermore, if I have the spare time and computing power, I would like to tweak my implementation(s) for improved performance over a standard deployment of one of these algorithms.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">Based on my research so far, You Only Look Once (YOLO) version 3 and Faster R-CNN are looking the most promising for this task; both claim to achieve comparably on metrics such as mAP as other top algorithms, while training or processing images in a fraction of the time, sometimes orders of magnitude less time.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">Standard Convolutional Neural Networks are not well suited to object detection tasks because the number of outputs can be variable from one image to another, thus requiring different output layer structures from one image to the next. &nbsp;One way to solve this would be to create many regions within an image that an object might be located within, then running the network on each region like a cropped picture. &nbsp;However, to be thorough, you&rsquo;d have to try impractically many regions in each image, which would get out of hand very quickly.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c5">R-CNN</span><sup class="c5"><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span class="c5">&nbsp;(see illustration below to the right) came along to help with that by limiting the number to 2000 regions, then reducing it further by grouping similar regions together using a selective search algorithm. &nbsp;This allowed you to only run a cropped image through the network if it was predicted to have a specific object in it. &nbsp;This was a huge step forward making object detection feasible, but it still was extremely slow.</span><hr style="page-break-before:always;display:none;"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 507.50px; height: 172.40px;"><img alt="" src="images/image4.png" style="width: 507.50px; height: 172.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c2">Next up, Fast R-CNN sped things up by first generating a feature map via the convolutional layers, then only running the region proposals generated from the feature map through the fully connected layers to predict the class and the specific bounding box coordinates, reducing the data run through the conv layers.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 300.60px; height: 307.50px;"><img alt="" src="images/image3.png" style="width: 300.60px; height: 307.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">Finally, Faster R-CNN (see illustration to the left from their paper) took this a step further by replacing the slow selective search with a faster object detection algorithm. &nbsp;A feature map is still created from feeding the image through the convolutional layers, but then a different network is used to draw the region proposals. &nbsp;Then they are reshaped in a RoI Pooling layer before being inputted into the fully connected classifier layers for specific bounding box and class predictions.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 431.00px; height: 270.49px;"><img alt="" src="images/image7.png" style="width: 431.00px; height: 270.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c5">Meanwhile, YOLO works by splitting the image into a grid of class probability while also drawing many random bounding boxes, then assigns probability to the boxes based on their alignment with the class map (see illustration to the right</span><sup class="c5"><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup><span class="c2">). &nbsp;While this allows it to work extremely quickly, it struggles with detecting objects that are smaller than the class probability map cells and it might struggle with higher numbers of classes like the 500 in this challenge.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">I&rsquo;ll begin by forking a Kernel that achieves modest results using an algorithm I&rsquo;m already familiar with to use as my baseline, as mentioned in the earlier Benchmark Model section. &nbsp;Then, I will try tweaking the parameters and model from the baseline, seeing how much improvement I can pull out of it in a reasonable amount of time. &nbsp;Once I&rsquo;ve reached an apparent practical limit, I&rsquo;ll move on and try a bleeding edge algorithm, like one of the ones previously mentioned. &nbsp;Depending on how training goes, I&rsquo;d like to either tweak it further to eek out better results and/or try another state-of-the-art algorithm. &nbsp;Along the way, I&rsquo;ll be evaluating the results and watching out for likely pitfalls in my model&rsquo;s performance.</span></p><p class="c1 c3"><span class="c2"></span></p><p class="c1"><span class="c2">Additionally, I could try steps like Image Augmentation if my available computing power seems to be handling training in reasonable amounts of time, but perhaps it wouldn&rsquo;t be necessary or feasible with such a large dataset. &nbsp;I also reserve the right to modify my strategy as the competition progresses and other Kagglers share strategies that are working for them. &nbsp;While Kaggle &lsquo;competitions&rsquo; are presented as such, they tend to be highly collaborative with people sharing what works and what doesn&rsquo;t work for them. &nbsp;While I have no illusions of placing near the top of the leaderboard in my first competition when up against teams of researchers that may be developing new, better algorithms specifically for this, I trust that my ability to research and collaborate with other Kagglers will lead me to a result I can be proud of.</span></p><hr class="c21"><div><p class="c10"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c4">&nbsp;</span><span class="c4 c18 c14 c27">Mountain Sheep by</span></p><p class="c10"><span class="c22 c4 c14"><a class="c0" href="https://www.google.com/url?q=https://www.flickr.com/people/hisgett/&amp;sa=D&amp;ust=1531459190687000">Tony Hisgett </a></span><span class="c4 c14 c18">(</span><span class="c22 c4 c14"><a class="c0" href="https://www.google.com/url?q=https://creativecommons.org/licenses/by/2.0/&amp;sa=D&amp;ust=1531459190687000">License</a></span><span class="c4 c18 c14">)</span></p></div><div><p class="c10"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c9">&nbsp;</span><span class="c4 c27 c18 c14">#goats #chivas #morning #bike #rider by</span></p><p class="c10"><span class="c22 c4 c14"><a class="c0" href="https://www.google.com/url?q=https://www.flickr.com/people/raybouk/&amp;sa=D&amp;ust=1531459190687000">Ray Bouknight </a></span><span class="c4 c18 c14">(</span><span class="c4 c14 c22"><a class="c0" href="https://www.google.com/url?q=https://creativecommons.org/licenses/by/2.0/&amp;sa=D&amp;ust=1531459190687000">License</a></span><span class="c4 c18 c14">)</span></p></div><div><p class="c10"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c9">&nbsp;</span><span class="c14 c9 c20">Koko with an eye by</span></p><p class="c10"><span class="c12 c9 c14"><a class="c0" href="https://www.google.com/url?q=https://www.linkedin.com/in/thenathanielwatkins/&amp;sa=D&amp;ust=1531459190688000">Nathaniel Watkins</a></span><span class="c14 c9">&nbsp;(</span><span class="c12 c14 c9"><a class="c0" href="https://www.google.com/url?q=https://creativecommons.org/licenses/by/2.0/&amp;sa=D&amp;ust=1531459190688000">License</a></span><span class="c20 c14 c9">)</span></p></div><div><p class="c10"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c9">&nbsp;</span><span class="c4 c14">Krasin I., Duerig T., Alldrin N., Ferrari V., Abu-El-Haija S., Kuznetsova A., Rom H., Uijlings J., Popov S., Kamali S., Malloci M., Pont-Tuset J., Veit A., Belongie S., Gomes V., Gupta A., Sun C., Chechik G., Cai D., Feng Z., Narayanan D., Murphy K</span><span class="c4">. OpenImages: A public dataset for large-scale multi-label and multi-class image classification, 2017. Available from </span><span class="c12 c4"><a class="c0" href="https://www.google.com/url?q=https://storage.googleapis.com/openimages/web/index.html&amp;sa=D&amp;ust=1531459190688000">https://storage.googleapis.com/openimages/web/index.html</a></span><span class="c4">.</span></p></div><div><p class="c10"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c9">&nbsp;</span><span class="c13 c9">Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) </span><span class="c9 c11">ImageNet Large Scale Visual Recognition Challenge</span><span class="c13 c9">. </span><span class="c13 c14 c9">IJCV,</span><span class="c13 c9">&nbsp;2015. </span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://link.springer.com/article/10.1007/s11263-015-0816-y?sa_campaign%3Demail/event/articleAuthor/onlineFirst%23&amp;sa=D&amp;ust=1531459190689000">paper</a></span><span class="c9 c13">&nbsp;| </span><span class="c9 c16"><a class="c0" href="https://www.google.com/url?q=http://ai.stanford.edu/~olga/bibtex/ILSVRC15.bib&amp;sa=D&amp;ust=1531459190689000">bibtex</a></span><span class="c13 c9">&nbsp;| </span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://arxiv.org/abs/1409.0575&amp;sa=D&amp;ust=1531459190689000">paper content on arxiv</a></span><span class="c13 c9">&nbsp;| </span><span class="c16 c9"><a class="c0" href="https://www.google.com/url?q=http://ai.stanford.edu/~olga/data/ILSVRC_annotations_ICCV13.zip&amp;sa=D&amp;ust=1531459190689000">attribute annotations</a></span></p></div><div><p class="c10"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c9">&nbsp;</span><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1405.0312&amp;sa=D&amp;ust=1531459190691000">arXiv:1405.0312</a></span><span class="c24">&nbsp;[cs.CV]</span></p></div><div><p class="c10"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c9">&nbsp;</span><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.00567&amp;sa=D&amp;ust=1531459190690000">arXiv:1512.00567</a></span><span class="c24">&nbsp;[cs.CV]</span></p></div><div><p class="c10"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c4">&nbsp;</span><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1804.02767&amp;sa=D&amp;ust=1531459190691000">arXiv:1804.02767</a></span><span class="c24">&nbsp;[cs.CV]</span></p></div><div><p class="c10"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c9">&nbsp;Image from Open Images Dataset V4 website: </span><span class="c12 c9"><a class="c0" href="https://www.google.com/url?q=https://storage.googleapis.com/openimages/web/object_detection_metric.html&amp;sa=D&amp;ust=1531459190690000">https://storage.googleapis.com/openimages/web/object_detection_metric.html</a></span></p><p class="c10 c3"><span class="c7"></span></p></div><div><p class="c10"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c9">&nbsp;</span><span class="c12 c9"><a class="c0" href="https://www.google.com/url?q=http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf&amp;sa=D&amp;ust=1531459190692000">http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf</a></span></p></div><div><p class="c10"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c9">&nbsp;</span><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.02325&amp;sa=D&amp;ust=1531459190692000">arXiv:1512.02325</a></span><span class="c24">&nbsp;[cs.CV]</span></p></div><div><p class="c10"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c9">&nbsp;</span><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1506.01497&amp;sa=D&amp;ust=1531459190693000">arXiv:1506.01497</a></span><span class="c24">&nbsp;[cs.CV]</span></p></div><div><p class="c10"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c9">&nbsp;</span><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/abs/1311.2524&amp;sa=D&amp;ust=1531459190693000">arXiv:1311.2524</a></span><span class="c24">&nbsp;[cs.CV]</span></p></div><div><p class="c10"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c9">&nbsp;YOLO illustration and a better overview of the algorithms: </span><span class="c12 c9"><a class="c0" href="https://www.google.com/url?q=https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e&amp;sa=D&amp;ust=1531459190693000">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></span></p></div></body></html>